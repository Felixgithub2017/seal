mt {
  inputDir = "data/LDC.1k"
  rootDir = "data/phrase"
  optType = ${ExtractType.Phrase}
  maxc = 5
  maxe = 5
  maxSpan = 10
  maxInitPhraseLength = 10
  maxSlots = 2
  minCWords = 1
  minEWords = 1

  maxCSymbols = 5
  maxESymbols = 5
  minWordsInSlot = 1

  allowConsecutive = false
  allowAllUnaligned = false
  c2eWord = "WordTranslationC2E"
  e2cWord = "WordTranslationE2C"

  //withNullOption this parameter can be Set true if need
  withNullOption= false
  allowBoundaryUnaligned = true
  cutoff = 0
  direction = ${Direction.C2E}

  # smoothing parameters
  smoothedWeight = 0.2
  allowGoodTuring = false
  allowKN = false
  allowMKN = false
  allowSmoothing = false

  #syntax based
  withSyntaxInfo = false
  depType = ${DepType.DepString}
  posType = ${POSType.AllPossible}
  constrainedType = ${ConstrainedType.WellFormed}
  syntax = ${SyntaxType.S2D}
  keepFloatingPOS = false
}

wam {
  source = "local source file"
  target = "local target file"
  alignedRoot = "hdfs root"
  redo = true
  iterations = 1
  nCPUs = 2
  memoryLimit = 64
  numCls = 50
  maxSentLen = 100
  totalReducer = 10
  splitNum = 1
  trainSeq ="1*H*3*"
  verbose = false
  overwrite = true
  nocopy = false

  # mgiza++ execution file
  d4norm  = "d4norm"
  hmmnorm = "hmmnorm"
  giza = "mgiza"
  symalbin = "symal"
  symalmethod = "grow-diag-final-and"
  symalscript = "symal.sh"
  giza2bal = "giza2bal.pl"

  filterlog = "filter-log"
  srcClass = ""
  tgtClass = ""
  encoding = "UTF-8"
}

lm {
  lmRootDir = "data/lm"
  lmInputFileName = "data/test.txt"
  N = 5
  splitDataRation = 0.01
  sampleNum = 10
  expectedErrorRate = 0.01
  offset = 100
  # smoothing method
  GT = false
  GSB = true
  KN = false
  MKN = false
}

# Spark parameter Settings
# http://spark.apache.org/docs/latest/configuration.html
sparkPara {
  partitionNum = 4
  totalCores = 4
  parallelism = 4
  registrationRequired = true
  # HDFS storage file conpress format
  # Lz4Codec and SnappyCodec need native build
  # "org.apache.hadoop.io.compress.BZip2Codec"
  #
  IOCompress = "org.apache.hadoop.io.compress.GzipCodec"

  driver {
    memory = 50
    cores = 8
    extraJavaOptions = "XX:+PrintGCDetails -XX:+PrintGCTimeStamps"
  }
  # here are some GC experiment parameters104
  # -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -Xms45g -Xmx45g
  #
  # G1 Garbage collection: http://www.oracle.com/technetwork/tutorials/tutorials-1876574.html
  #
  # -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1Summarize ConcMark -Xms45g -Xmx45g
  # -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1Summarize ConcMark -Xms45g -Xmx45g -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20
  # -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1Summarize ConcMark -Xms45g -Xmx45g -XX:NewRation=8
  # -XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1Summarize ConcMark
  #
  executor {
    num = 10
    memory = 22
    cores = 8
    extraJavaOptions = "-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1Summarize ConcMark"
  }

  shuffle {
    reducer.maxSizeInFlight = 48
    compress = true
    compression.codec = "lz4"
    file.buffer = 32
  }

  rdd {
    compress = false
  }

  memoryManagement {
    # The lower this is, the more frequently spills and cached data eviction occur.
    offHeap.enabled = false
    offHeap.size = "12MB"
    memory.fraction = 0.6
    memory.storageFraction = 0.5
    memory.useLegacyMode = false
    storage.memoryFraction = 0.6
    shuffle.memoryFraction = 0.2
  }
}

Direction {
  E2C = "E2C"
  C2E = "C2E"
  All = "All"
}
SyntaxType {
  S2D = "S2D"
  T2S ="T2S"
  S2T = "S2T"
  S2S = "S2S"
  None = "NULL"
}
ExtractType {
  Phrase = "Phrase"
  Rule = "Rule"
  PhraseWithReordering = "PhraseWithReordering"
}
#
# there are three levels of hard syntax constraint
# 1: every span should be syntactically well-formed
# 2: the span could be relaxed well-formed
# 3: the span could be any continuous phrase
#
ConstrainedType {
  WellFormed = "WellFormed"
  Relaxed = "Relaxed"
  ANY = "ANY"
}
#
# parameters about the target dependency structure, i.e. S2D
# there are several modes for using target side dependency structure
# 1: using the dependency string
# 2: using the postag of each span only
#
DepType {
  DepString = "DepString"
  None = "None"
}

#
# 1. AllPossible : aggregate all possible pos-tags for each span
# 2. BestOne : use the pos-tag with the largest count
# 3. Frame : use the pos-tag for the head of the span and the head of each sub spans as a frame
# 4. None : do not use postag
#
POSType {
  AllPossible = "AllPossible"
  BestOne = "BestOne"
  Frame = "Frame"
  None = "None"
}

